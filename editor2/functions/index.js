/**
 * Import function triggers from their respective submodules:
 *
 * const {onCall} = require("firebase-functions/v2/https");
 * const {onDocumentWritten} = require("firebase-functions/v2/firestore");
 *
 * See a full list of supported triggers at https://firebase.google.com/docs/functions
 */


const {initializeApp} = require("firebase-admin/app");
const logger = require("firebase-functions/logger");
const path = require("path");

const {getFirestore} = require("firebase-admin/firestore");

const {onCall, HttpsError} = require("firebase-functions/v2/https");

const {
  onObjectFinalized,
  onObjectDeleted,
  onMetadataUpdated,
  onObjectArchived
} = require("firebase-functions/v2/storage");
const {
  onDocumentWritten,
  onDocumentCreated,
  onDocumentUpdated,
  onDocumentDeleted
} = require("firebase-functions/v2/firestore");

initializeApp();
const db = getFirestore();

exports.onObjectFinalized = onObjectFinalized({}, async (event) => {
  const {bucket, name, contentType, timeCreated, metadata} = event.data
  db.doc(`bucket/${bucket}/objects/${name}`).set({
    contentType,
    timeCreated,
    ...metadata
  })
})

const {Storage} = require('@google-cloud/storage');
const storage = new Storage();

async function generateV4ReadSignedUrl(bucketName, fileName) {
  // These options will allow temporary read access to the file
  const options = {
    version: 'v4',
    action: 'write',
    expires: Date.now() + 7 * 24 * 60 * 60 * 1000, // 7 days
  };

  // Get a v4 signed URL for reading the file
  const [url] = await storage
    .bucket(bucketName)
    .file(fileName)
    .getSignedUrl(options);

  console.log('Generated GET signed URL:');
  console.log(url);
  console.log('You can use this URL with any user agent, for example:');
  console.log(`curl '${url}'`);
}

exports.createSignedPUTUrl = onCall(async (request) => {
  // request.data is a string "{scopeKind}/{scope}/file/{fileId}"
  // eg. user/123/files/abc
  //     group/123/files/abc
  //     doc/123/files/abc



  // parse and destructure the string into variables
  const [scopeKind, scope, _, fileId] = request.data.split('/')
  const bucket = storage.bucket()
})

// exports.myfunction = onDocumentWritten("my-collection/{docId}", (event) => {
//    /* ... */
// onCreate const snapshot = event.data // then snapshot.data();
// onUpdate const newSnapshot = event.data.after // then newSnapshot.data();
// onDelete const snapshot = event.data // then snapshot.data();
// });

// Create and deploy your first functions
// https://firebase.google.com/docs/functions/get-started

// exports.helloWorld = onRequest((request, response) => {
//   logger.info("Hello logs!", {structuredData: true});
//   response.send("Hello from Firebase!");
// });

// when a new file is written to the default bucket,
// write a new document to the firestore "bucket/{bucketId}/objects/{objectId}" collection
// with the name, size, timeCreated, and other metadata of the file


// let's back up a bit and think about where these files come from,
// so that we can know how to organize them and set up security rules.
// Users are editing a dock. They are working on debugging something, and they want to see the
// thing that they bugging on their screen. They set up a watch
// On a directory and every file that goes in that directory gets uploaded.

// 1. user sets up a watch on a directory, they save files there,
//    - automatically uploaded to GCS
//    - metadata is synced to Firestore: visible in sci.town

// (defbucket !my-stuff)

// - UI prompts creation of the bucket. This then re-writes the source to include
//   an ID.

// (defbucket !my-stuff {:multi true :id "123"})

// SDKs in all the Gen languages.



// - user can manually or programmatically get write-enabled signed URLs.
//   - the request includes the desired filename. It can overwrite a file if it already exists.
//     the bucket is versioned with 7 day retention.
// - data is scoped to the doc. The doc is the scope. If the doc is copied into a new scope,
//   the data won't be visible because it is still subject to the old scope's security rules.
//   the bucket remembers which doc it was created in. If it "finds itself" in a new doc, it
//   may be viewable (if the current user has permission to view the original doc). But no writable
//   URLs/tokens should be generated by this widget.
// -

// sci.town visibility
// - show _your_ files by _recency_ OR
// - open a doc
//   - create a (bucket :my-stuff :multi true).
//     Returns a snippet for uploading any number of files to this location.
//   - create a (bucket :traces ). get a "doc" reference. allows uploading files to be appended to

// all the ways we could get traces into sci.town:

// - mirror a directory to GCS
// - upload via curl
// - upload via Julia/Python sdks (or any other language)
// - upload via rsync
// - upload by dragging and dropping into the browser
// - a VSCode extension that directly visualizes the traces
// - something like tap> in clojure

// Workflows:

// Notebook Integration: If developers are using Jupyter notebooks or similar tools for their machine learning work, providing a way to directly upload traces from within the notebook could be very convenient. This could be a Python library that they can import and use within their code.
// CLI Tool: A command-line tool that can upload traces to the bucket could be useful. This tool could watch a directory for new trace files and upload them automatically, or it could be used to manually upload specific files.
// IDE Integration: If developers are using a specific IDE for their work, a plugin for that IDE that can upload traces could be very useful. This could be integrated into the IDE's UI, or it could be a set of commands that can be run from within the IDE.
// API: Providing an API that developers can use to upload traces from their own code could be very flexible and allow for a lot of different workflows.

// Questions to Ask Developers:

// What tools are you currently using for your machine learning work? (e.g., specific programming languages, libraries, IDEs, notebooks)
// How are you currently generating and storing traces?
// What is your current workflow for visualizing these traces?
// What pain points do you have with your current workflow?
// Would you prefer a solution that integrates with your existing tools (e.g., a library for your programming language, a plugin for your IDE), or would you be open to using a separate tool (e.g., a web interface, a CLI tool)?
// How important is automation in your workflow? Would you prefer a solution that automatically uploads new traces, or would you prefer to have more control over when and what gets uploaded?

